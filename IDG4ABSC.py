{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPk6ItJlJjhjkKsc1aP5Oh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sara-kaczmarek/LLMB4ABSC/blob/main/IDG4ABSC.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKPLkbBbE5U0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "###### ASPECT EXPANSION (AX) ######\n",
        "def ax_prompt(aspect, domain, model, tokenizer, max_tokens=30, device=\"cuda\"):\n",
        "    prompt = (\n",
        "        f\"You are an expert in Aspect-Based Sentiment Analysis for the {domain} domain.\\n\"\n",
        "        \"You will perform an Aspect Expansion task. In this task, you are given an aspect term and your goal is to generate related terms that customers may use to refer to the same aspect.\\n\"\n",
        "        \"Generate 3 to 5 related terms, including synonyms, homonyms or alternative expressions.\\n\"\n",
        "        \"Output format: [term1, term2, term3, term4, term5]\\n\\n\"\n",
        "        \"If fewer than 5 related terms exist, output as many as applicable.\\n\\n\"\n",
        "        f\"Aspect term: {aspect}\\n\"\n",
        "        \"Output:\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.0,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    expanded_text = decoded.replace(prompt, \"\").strip()\n",
        "    return expanded_text\n",
        "\n",
        "def extract_and_parse_list(generated_text):\n",
        "    match = re.search(r\"\\[(.*?)\\]\", generated_text)\n",
        "    if not match:\n",
        "        return None\n",
        "    items = match.group(1).split(\",\")\n",
        "    return [item.strip().strip('\"').strip(\"'\") for item in items if item.strip()]\n",
        "\n",
        "def generate_aspect_expansions(df, model, tokenizer, domain=\"restaurant\", device=\"cuda\"):\n",
        "\n",
        "    unique_aspects = df[\"aspect\"].dropna().unique().tolist()\n",
        "\n",
        "    records = []\n",
        "\n",
        "    for aspect in tqdm(unique_aspects, desc=\"Generating expansions\"):\n",
        "        expanded_text = ax_prompt(aspect, domain, model, tokenizer, device=device)\n",
        "\n",
        "        expanded_list = extract_and_parse_list(expanded_text)\n",
        "        if expanded_list is None:\n",
        "            expanded_list = []\n",
        "\n",
        "        expanded_list.append(aspect)\n",
        "\n",
        "        seen = set()\n",
        "        cleaned = []\n",
        "        for x in expanded_list:\n",
        "            x = x.strip().lower()\n",
        "            if x not in seen:\n",
        "                seen.add(x)\n",
        "                cleaned.append(x)\n",
        "\n",
        "        records.append((aspect, cleaned))\n",
        "\n",
        "    df = pd.DataFrame(records, columns=[\"original_aspect\", \"expanded_aspects\"])\n",
        "\n",
        "    df[\"expanded_aspects\"] = df[\"expanded_aspects\"].apply(lambda x: \", \".join(x))\n",
        "\n",
        "    return df\n",
        "\n",
        "def filter_expanded_aspects_to_nouns(df, nlp):\n",
        "    def is_valid_noun(phrase):\n",
        "        phrase = phrase.strip()\n",
        "        if not phrase:\n",
        "            return False\n",
        "        doc = nlp(phrase)\n",
        "        return any(token.pos_ in {\"NOUN\", \"PROPN\"} for token in doc)\n",
        "\n",
        "    def filter_nouns(row):\n",
        "        original = row[\"original_aspect\"].strip()\n",
        "        terms = [term.strip() for term in row[\"expanded_aspects\"].split(\",\")]\n",
        "        valid = [term for term in terms if is_valid_noun(term)]\n",
        "\n",
        "        if original not in valid:\n",
        "            valid.append(original)\n",
        "\n",
        "        return \", \".join(sorted(set(valid), key=valid.index))\n",
        "\n",
        "    df[\"expanded_aspects\"] = df.apply(filter_nouns, axis=1)\n",
        "    return df\n",
        "\n",
        "###### DATA GENERATION (DG) ######\n",
        "def dg_prompt(aspect, sentiment, domain, model, tokenizer, max_tokens=50, device=\"cuda\"):\n",
        "    neutral_hint = (\n",
        "        \" For neutral sentiment, do not express any opinion, use factual or descriptive language, and avoid emotional or judgmental words.\"\n",
        "        if sentiment.lower() == \"neutral\" else \"\"\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are generating a single {domain} review sentence.\\n\"\n",
        "        \"The sentence must mention the given aspect and reflect the given sentiment.\\n\"\n",
        "        \"Use natural human-like language, realistic and rich in vocabulary.\\n\"\n",
        "        \"Output format: one sentence of maximum 20 words. Stop generating after 1 sentence.\" +\n",
        "        neutral_hint + \"\\n\" +\n",
        "        f\"Aspect: {aspect}\\n\"\n",
        "        f\"Sentiment: {sentiment}\\n\"\n",
        "        \"Output:\"\n",
        "    )\n",
        "\n",
        "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in encoded.items()}\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.9,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=1.2,\n",
        "        no_repeat_ngram_size=3\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    generated_text = decoded.replace(prompt, \"\").strip().split(\"\\n\")[0].strip()\n",
        "    return clean_generated_sentence(generated_text)\n",
        "\n",
        "def clean_generated_sentence(text):\n",
        "    if pd.isna(text):\n",
        "        return None\n",
        "\n",
        "    text = str(text).split('\\n')[0].strip()\n",
        "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'[^\\w\\s.,!?\\'()-]', '', text)\n",
        "    text = re.sub(r'^[\\'\"“”‘’]+|[\\'\"“”‘’]+$', '', text)\n",
        "\n",
        "    match = re.split(r'(?<=[.!?])\\s', text, maxsplit=1)\n",
        "    text = match[0]\n",
        "\n",
        "    return text if text else None\n",
        "\n",
        "def generate_synthetic_sentences_from_expansions(df_expansions, dataset_choice, model, tokenizer, n_per_sentiment=1, device=\"cuda\"):\n",
        "    sentiments = [\"positive\", \"neutral\", \"negative\"]\n",
        "    domain = \"laptop\" if dataset_choice == \"Lapt14\" else \"restaurant\"\n",
        "    results = []\n",
        "\n",
        "    for _, row in tqdm(df_expansions.iterrows(), total=len(df_expansions), desc=\"Generating Synthetic Sentences\"):\n",
        "        original = row[\"original_aspect\"]\n",
        "        expansions = [term.strip() for term in row[\"expanded_aspects\"].split(\",\") if term.strip()]\n",
        "\n",
        "        for sentiment in sentiments:\n",
        "            for _ in range(n_per_sentiment):\n",
        "                used = random.choice(expansions) if expansions else original\n",
        "\n",
        "                gen_text = dg_prompt(\n",
        "                    aspect=used,\n",
        "                    sentiment=sentiment,\n",
        "                    domain=domain,\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                results.append({\n",
        "                    \"original_aspect\": original,\n",
        "                    \"used_extended_aspect\": used,\n",
        "                    \"sentiment\": sentiment,\n",
        "                    \"generated_sentence\": gen_text\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "########## QUALITY FILTERING PIPELINE (QFP) ###############\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"This pattern is interpreted as a regular expression\")\n",
        "\n",
        "def manual_filter(df):\n",
        "    df = df.copy()\n",
        "    df[\"reason_dropped\"] = \"\"\n",
        "\n",
        "    df[\"generated_sentence\"] = df[\"generated_sentence\"].fillna(\"\").astype(str)\n",
        "\n",
        "    subjective_words = [\n",
        "        \"good\", \"great\", \"delicious\", \"amazing\", \"fantastic\", \"excellent\",\n",
        "        \"bad\", \"awful\", \"horrible\", \"terrible\", \"lovely\", \"awesome\",\n",
        "        \"tasty\", \"perfect\", \"nice\", \"worst\", \"wonderful\", \"love\", \"hate\",\n",
        "        \"disgusting\", \"incredible\", \"favorite\", \"boring\", \"unpleasant\",\n",
        "        \"yummy\", \"satisfying\", \"flavorful\", \"perfection\", \"perfectly\",\n",
        "        \"cozy\", \"welcoming\", \"inviting\", \"authentic\", \"bold\", \"rich\", \"original\",\n",
        "        \"interesting\", \"relaxed\", \"appealing\", \"iconic\", \"classic\",\n",
        "        \"well seasoned\", \"well cooked\", \"popular\", \"skilled\", \"recommended\",\n",
        "        \"recommend\", \"bland\", \"quiet\", \"clean\", \"quick service\", \"steep prices\",\n",
        "        \"discerning palate\", \"would order again\", \"famous\",\"perfection\",\"perfectly\"\n",
        "    ]\n",
        "    pattern = r'\\b(?:' + '|'.join(map(re.escape, subjective_words)) + r')\\b'\n",
        "\n",
        "    mask_missing = df[\"generated_sentence\"].str.strip() == \"\"\n",
        "    df.loc[mask_missing, \"reason_dropped\"] = \"missing_or_empty\"\n",
        "\n",
        "    dup_mask = df.duplicated(subset=[\"generated_sentence\"]) & df[\"reason_dropped\"].eq(\"\")\n",
        "    df.loc[dup_mask, \"reason_dropped\"] = \"duplicate\"\n",
        "\n",
        "    punct_mask = ~df[\"generated_sentence\"].str.strip().str.endswith(tuple(\".!?\")) & df[\"reason_dropped\"].eq(\"\")\n",
        "    df.loc[punct_mask, \"reason_dropped\"] = \"no_end_punctuation\"\n",
        "\n",
        "    short_mask = df[\"generated_sentence\"].str.split().str.len() < 2\n",
        "    short_mask = short_mask & df[\"reason_dropped\"].eq(\"\")\n",
        "    df.loc[short_mask, \"reason_dropped\"] = \"too_short\"\n",
        "\n",
        "    is_neutral = df[\"sentiment\"].str.lower() == \"neutral\"\n",
        "    contains_subjective = df[\"generated_sentence\"].str.lower().str.contains(pattern, na=False, regex=True)\n",
        "    sub_mask = is_neutral & contains_subjective & df[\"reason_dropped\"].eq(\"\")\n",
        "    df.loc[sub_mask, \"reason_dropped\"] = \"subjective_in_neutral\"\n",
        "\n",
        "    dropped_df = df[df[\"reason_dropped\"] != \"\"].reset_index(drop=True)\n",
        "    filtered_df = df[df[\"reason_dropped\"] == \"\"].drop(columns=[\"reason_dropped\"]).reset_index(drop=True)\n",
        "\n",
        "    return filtered_df, dropped_df\n",
        "\n",
        "\n",
        "def llm_filter(df, model, tokenizer, domain, device=\"cuda\"):\n",
        "    from collections import defaultdict\n",
        "    import pandas as pd\n",
        "    from tqdm import tqdm\n",
        "    import re\n",
        "\n",
        "    def extract_decision(text, label=\"\"):\n",
        "        match = re.search(r\"(?i)answer:\\s*(yes|no)\", text, re.DOTALL)\n",
        "        if match:\n",
        "            decision = match.group(1).capitalize()\n",
        "\n",
        "            return decision\n",
        "        else:\n",
        "\n",
        "            return None\n",
        "\n",
        "    drop_counts = defaultdict(int)\n",
        "    results = []\n",
        "\n",
        "    for i, (_, row) in enumerate(tqdm(df.iterrows(), total=len(df), desc=\"Applying filters\")):\n",
        "        sentence = row[\"generated_sentence\"]\n",
        "        aspect = row[\"used_extended_aspect\"]\n",
        "        sentiment = row[\"sentiment\"]\n",
        "        original_aspect = row.get(\"original_aspect\", None)\n",
        "\n",
        "\n",
        "        row_result = {\n",
        "            \"generated_sentence\": sentence,\n",
        "            \"original_aspect\": original_aspect,\n",
        "            \"used_extended_aspect\": aspect,\n",
        "            \"sentiment\": sentiment,\n",
        "            \"aspect_sentiment_match\": None,\n",
        "            \"domain_match\": None,\n",
        "            \"english_fluent\": None\n",
        "        }\n",
        "\n",
        "        combined_prompt = (\n",
        "            f\"Does this sentence express a {sentiment} sentiment towards the aspect '{aspect}'?\\n\"\n",
        "            f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
        "            f\"Respond with either Yes or No. No explanations.\\nAnswer:\"\n",
        "        )\n",
        "        inputs = tokenizer(combined_prompt, return_tensors=\"pt\").to(device)\n",
        "        output = model.generate(**inputs, max_new_tokens=10, temperature=0.0,\n",
        "                                do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "        combined_response = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        combined_decision = extract_decision(combined_response, \"Aspect+Sentiment\")\n",
        "        row_result[\"aspect_sentiment_match\"] = combined_decision\n",
        "\n",
        "        if combined_decision is None:\n",
        "            drop_counts[\"aspect_sentiment_no_answer\"] += 1\n",
        "            continue\n",
        "        elif combined_decision != \"Yes\":\n",
        "            drop_counts[\"aspect_sentiment_answered_no\"] += 1\n",
        "            continue\n",
        "\n",
        "        if sentiment.lower() == \"neutral\":\n",
        "            neutral_prompt = (\n",
        "                f\"Is there any emotional or opinionated expression toward the aspect '{aspect}' in this sentence?\\n\"\n",
        "                f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
        "                f\"Respond with either Yes or No. No explanations.\\nAnswer:\"\n",
        "            )\n",
        "            inputs = tokenizer(neutral_prompt, return_tensors=\"pt\").to(device)\n",
        "            output = model.generate(**inputs, max_new_tokens=10, temperature=0.0,\n",
        "                                    do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "            neutral_response = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "\n",
        "            neutral_decision = extract_decision(neutral_response, \"Neutral Emotion Check\")\n",
        "\n",
        "            if neutral_decision is None:\n",
        "                drop_counts[\"neutral_emotion_no_answer\"] += 1\n",
        "                continue\n",
        "            elif neutral_decision == \"Yes\":\n",
        "                drop_counts[\"neutral_emotion_expressed\"] += 1\n",
        "                continue\n",
        "\n",
        "        df_prompt = (\n",
        "            f\"Does the following sentence sound like something a customer would write in a {domain} review?\\n\"\n",
        "            f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
        "            f\"Respond with either Yes or No. No explanations.\\nAnswer:\"\n",
        "        )\n",
        "        inputs = tokenizer(df_prompt, return_tensors=\"pt\").to(device)\n",
        "        output = model.generate(**inputs, max_new_tokens=10, temperature=0.0,\n",
        "                                do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "        df_response = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        df_decision = extract_decision(df_response, \"Domain\")\n",
        "        row_result[\"domain_match\"] = df_decision\n",
        "\n",
        "        if df_decision is None:\n",
        "            drop_counts[\"domain_no_answer\"] += 1\n",
        "            continue\n",
        "        elif df_decision != \"Yes\":\n",
        "            drop_counts[\"domain_answered_no\"] += 1\n",
        "            continue\n",
        "\n",
        "        ef_prompt = (\n",
        "            f\"Is the following sentence fluent and grammatically correct English?\\n\"\n",
        "            f\"Sentence: \\\"{sentence}\\\"\\n\"\n",
        "            f\"Respond with either Yes or No. No explanations.\\nAnswer:\"\n",
        "        )\n",
        "        inputs = tokenizer(ef_prompt, return_tensors=\"pt\").to(device)\n",
        "        output = model.generate(**inputs, max_new_tokens=10, temperature=0.0,\n",
        "                                do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "        ef_response = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        ef_decision = extract_decision(ef_response, \"English Fluency\")\n",
        "        row_result[\"english_fluent\"] = ef_decision\n",
        "\n",
        "        if ef_decision is None:\n",
        "            drop_counts[\"fluency_no_answer\"] += 1\n",
        "            continue\n",
        "        elif ef_decision != \"Yes\":\n",
        "            drop_counts[\"fluency_answered_no\"] += 1\n",
        "            continue\n",
        "\n",
        "        results.append(row_result)\n",
        "\n",
        "    df_filtered = pd.DataFrame(results).reset_index(drop=True)\n",
        "    df_final = df_filtered.drop(columns=[\n",
        "        \"aspect_sentiment_match\",\n",
        "        \"domain_match\",\n",
        "        \"english_fluent\"\n",
        "    ])\n",
        "\n",
        "    df_drop_stats = pd.DataFrame(list(drop_counts.items()), columns=[\"Stage\", \"Dropped\"])\n",
        "    return df_final, df_drop_stats\n",
        "\n",
        "########## ITERATIVE DATA GENERATION (IDG) ###############\n",
        "\n",
        "def iterative_DG(\n",
        "    df_synthetic_filtered,\n",
        "    df_expansions_filtered,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    dataset_choice,\n",
        "    manual_filter,\n",
        "    llm_filter,\n",
        "    n_per_sentiment=10,\n",
        "    max_attempts=10,\n",
        "    device=\"cuda\"\n",
        "):\n",
        "    sentiments = [\"positive\", \"neutral\", \"negative\"]\n",
        "    domain = \"laptop\" if dataset_choice == \"Lapt14\" else \"restaurant\"\n",
        "    total_target = len(df_expansions_filtered) * len(sentiments) * n_per_sentiment\n",
        "\n",
        "    icl_pool = df_synthetic_filtered.drop_duplicates(subset=[\"generated_sentence\"]).copy()\n",
        "    results = [icl_pool.copy()]\n",
        "    global_counter = len(icl_pool)\n",
        "\n",
        "    for _, row in tqdm(df_expansions_filtered.iterrows(), total=len(df_expansions_filtered), desc=\"Generating Balanced Data\"):\n",
        "        original_aspect = row[\"original_aspect\"]\n",
        "        expanded_list = [x.strip() for x in row[\"expanded_aspects\"].split(\",\") if x.strip()]\n",
        "        if not expanded_list:\n",
        "            expanded_list = [original_aspect]\n",
        "\n",
        "        for sentiment in sentiments:\n",
        "            current_count = len(icl_pool[(icl_pool[\"original_aspect\"] == original_aspect) & (icl_pool[\"sentiment\"] == sentiment)])\n",
        "            attempts = 0\n",
        "\n",
        "            while current_count < n_per_sentiment and attempts < max_attempts:\n",
        "                demo_pool = icl_pool[\n",
        "                    ((icl_pool[\"original_aspect\"] != original_aspect) & (icl_pool[\"sentiment\"] == sentiment)) |\n",
        "                    ((icl_pool[\"original_aspect\"] == original_aspect) & (icl_pool[\"sentiment\"] != sentiment))\n",
        "                ]\n",
        "                if demo_pool.empty:\n",
        "                    break\n",
        "\n",
        "                demos = demo_pool.sample(n=min(5, len(demo_pool)), random_state=random.randint(0, 10000))\n",
        "\n",
        "                prompt = (\n",
        "                    f\"You are generating a single {domain} review sentence.\\n\"\n",
        "                    \"Each sentence must mention the given aspect and reflect the given sentiment.\\n\"\n",
        "                    \"Use natural human-like language, realistic and rich in vocabulary.\\n\"\n",
        "                )\n",
        "\n",
        "                if sentiment.lower() == \"neutral\":\n",
        "                    prompt += \"The sentence must express no clear opinion or emotion. It should be factual or descriptive only.\\n\"\n",
        "\n",
        "                prompt += \"Output format: one sentence of maximum 20 words. Stop generating after 1 sentence.\\n\\n\"\n",
        "\n",
        "                for _, demo in demos.iterrows():\n",
        "                    prompt += (\n",
        "                        f\"Aspect: {demo['used_extended_aspect']}\\n\"\n",
        "                        f\"Sentiment: {demo['sentiment']}\\n\"\n",
        "                        f\"Output: {demo['generated_sentence']}\\n\\n\"\n",
        "                    )\n",
        "\n",
        "                aspect_used = random.choice(expanded_list)\n",
        "                prompt += (\n",
        "                    f\"Aspect: {aspect_used}\\n\"\n",
        "                    f\"Sentiment: {sentiment}\\n\"\n",
        "                    \"Output:\"\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "                    outputs = model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=50,\n",
        "                        temperature=0.9,\n",
        "                        top_p=0.9,\n",
        "                        do_sample=True,\n",
        "                        pad_token_id=tokenizer.eos_token_id,\n",
        "                        repetition_penalty=1.2,\n",
        "                        no_repeat_ngram_size=3\n",
        "                    )\n",
        "\n",
        "                    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                    generated = decoded.replace(prompt, \"\").strip().split(\"\\n\")[0].strip()\n",
        "                    cleaned = clean_generated_sentence(generated)\n",
        "\n",
        "                    if not cleaned:\n",
        "                        attempts += 1\n",
        "                        continue\n",
        "\n",
        "                    candidate_df = pd.DataFrame([{\n",
        "                        \"original_aspect\": original_aspect,\n",
        "                        \"used_extended_aspect\": aspect_used,\n",
        "                        \"sentiment\": sentiment,\n",
        "                        \"generated_sentence\": cleaned\n",
        "                    }])\n",
        "\n",
        "                    candidate_df, _ = manual_filter(candidate_df)\n",
        "                    if len(candidate_df) == 0:\n",
        "                        attempts += 1\n",
        "                        continue\n",
        "\n",
        "                    filtered_df, _ = llm_filter(candidate_df, model, tokenizer, domain, device)\n",
        "                    if not filtered_df.empty:\n",
        "                        results.append(filtered_df)\n",
        "                        icl_pool = pd.concat([icl_pool, filtered_df], ignore_index=True)\n",
        "                        current_count += 1\n",
        "                        global_counter += 1\n",
        "                        print(f\"[Accepted: {global_counter}/{total_target}] — ({original_aspect}, {sentiment})\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"[ERROR] {e}\")\n",
        "                attempts += 1\n",
        "\n",
        "    final_df = pd.concat(results).reset_index(drop=True)\n",
        "    return final_df"
      ]
    }
  ]
}